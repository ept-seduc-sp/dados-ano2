{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "804f4268",
   "metadata": {},
   "source": [
    "# Aula 3 — Estrutura básica de um script PySpark\n",
    "\n",
    "Neste notebook, realizamos o primeiro contato com código PySpark.\n",
    "O objetivo é compreender a estrutura básica de um script e a lógica dos comandos iniciais.\n",
    "\n",
    "O foco está em:\n",
    "- criar dados com `spark.range()`\n",
    "- entender que isso gera um DataFrame\n",
    "- visualizar os dados com `show()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6446516b",
   "metadata": {},
   "source": [
    "## Observação importante (ambiente local)\n",
    "\n",
    "Para o Spark funcionar corretamente neste ambiente local, é necessário que o **Java esteja instalado**.\n",
    "Recomendação: **Java 17 ou superior**.\n",
    "\n",
    "Se o Spark não iniciar ou travar na criação da SparkSession, verifique a versão do Java."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12764fce",
   "metadata": {},
   "source": [
    "## Usando o Spark no Jupyter\n",
    "\n",
    "Para garantir que o código funcione em qualquer ambiente, vamos criar uma SparkSession local e leve.\n",
    "Isso evita dependências de configurações prévias do Jupyter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d09e09e",
   "metadata": {},
   "source": [
    "## Criando uma SparkSession leve\n",
    "\n",
    "Vamos iniciar o Spark em modo local apenas para fins didáticos. Uma SparkSession é o ponto de entrada principal do Apache Spark, que concentra as configurações e oferece acesso unificado a recursos como DataFrame e SQL. Ela deve ser criada no início porque inicializa o ambiente de execução do Spark e garante que todos os recursos (contexto, configurações e conectores) estejam disponíveis de forma consistente para o restante do código.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "spark-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Aula3\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0021369",
   "metadata": {},
   "source": [
    "## Criando um DataFrame com `spark.range(0, 10)`\n",
    "\n",
    "O comando `spark.range(início, fim)` cria um DataFrame automaticamente.\n",
    "\n",
    "- `início`: valor inicial\n",
    "- `fim`: valor final (**não incluído**)\n",
    "\n",
    "Então `spark.range(0, 10)` gera valores de **0 a 9**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c715bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.range(0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacd41bf",
   "metadata": {},
   "source": [
    "## Visualizando os dados com show()\n",
    "\n",
    "Criar um DataFrame não exibe automaticamente os dados.\n",
    "Para visualizar o conteúdo, utilizamos o método `show()`.\n",
    "\n",
    "Esse comando exibe as linhas do DataFrame no ambiente de execução, permitindo observar os valores gerados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ccac7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bcda32",
   "metadata": {},
   "source": [
    "## Expandindo: `range` com variáveis\n",
    "\n",
    "Em vez de escrever os números direto no `range()`, podemos usar variáveis.\n",
    "Isso ajuda a entender que o intervalo pode mudar conforme a necessidade.\n",
    "\n",
    "Exemplo:\n",
    "- `inicio = 3`\n",
    "- `fim = 8`\n",
    "\n",
    "Isso deve gerar valores de **3 a 7** (porque o fim não entra).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13a84321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inicio = 3\n",
    "fim = 8\n",
    "\n",
    "df_var = spark.range(inicio, fim)\n",
    "df_var.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d38356",
   "metadata": {},
   "source": [
    "## Criando mais colunas no DataFrame\n",
    "\n",
    "Até agora, nosso DataFrame possui apenas uma coluna (`id`),\n",
    "gerada automaticamente pelo `spark.range()`.\n",
    "\n",
    "É possível criar **novas colunas** a partir dessa coluna,\n",
    "sem criar um novo DataFrame do zero.\n",
    "\n",
    "Isso permite enriquecer os dados e trabalhar com tabelas\n",
    "com mais de uma coluna.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd2888ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----------+\n",
      "| id|id_dobro|id_quadrado|\n",
      "+---+--------+-----------+\n",
      "|  0|       0|          0|\n",
      "|  1|       2|          1|\n",
      "|  2|       4|          4|\n",
      "|  3|       6|          9|\n",
      "|  4|       8|         16|\n",
      "|  5|      10|         25|\n",
      "|  6|      12|         36|\n",
      "|  7|      14|         49|\n",
      "|  8|      16|         64|\n",
      "|  9|      18|         81|\n",
      "+---+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_multi = (\n",
    "    df\n",
    "    .withColumn(\"id_dobro\", F.col(\"id\") * 2)\n",
    "    .withColumn(\"id_quadrado\", F.col(\"id\") * F.col(\"id\"))\n",
    ")\n",
    "\n",
    "df_multi.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8cf613",
   "metadata": {},
   "source": [
    "Neste exemplo:\n",
    "- `id_dobro` é o valor da coluna `id` multiplicado por 2\n",
    "- `id_quadrado` é o valor da coluna `id` ao quadrado\n",
    "\n",
    "Agora o DataFrame possui **três colunas**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f02e2a",
   "metadata": {},
   "source": [
    "## Estrutura básica de um script PySpark\n",
    "\n",
    "O fluxo mínimo de um script PySpark envolve:\n",
    "\n",
    "1. Importar as bibliotecas necessárias\n",
    "2. Criar a SparkSession\n",
    "3. Criar dados (DataFrame)\n",
    "4. Visualizar ou manipular os dados\n",
    "\n",
    "Essa estrutura será reutilizada em outros exemplos ao longo do curso.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
