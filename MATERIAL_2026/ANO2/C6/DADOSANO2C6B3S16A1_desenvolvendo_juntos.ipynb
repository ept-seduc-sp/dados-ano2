{
          "cells": [
                    {
                              "cell_type": "markdown",
                              "id": "83b5aef7",
                              "metadata": {},
                              "source": [
                                        "<h1 style=\"text-align: center;\">TÉCNICO EM CIÊNCIA DE DADOS</h1>\n",
                                        "<h1 style=\"text-align: center;\">Roteiro do Desenvolvendo Juntos</h1>\n",
                                        "<br>\n",
                                        "<br>\n",
                                        "\n",
                                        "**Componente:** Fundamentos de Ambientes e Arquitetura de Dados\n",
                                        "<br>\n",
                                        "**Unidade Curricular:** Introdução ao PySpark e Processamento Distribuído\n",
                                        "<br>\n",
                                        "**Tema da Semana:** Iniciando com DataFrames\n",
                                        "<br>\n",
                                        "**Semana 16 – Aula 1**\n",
                                        "<br>\n",
                                        "**Aula 1:** SparkSession e DataFrames\n",
                                        "\n",
                                        "<h2 align=\"center\">SparkSession e DataFrame</h2>"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "f0cf7b62",
                              "metadata": {},
                              "source": [
                                        "## Objetivos\n",
                                        "\n",
                                        "- Compreender o papel da SparkSession.\n",
                                        "- Criar um DataFrame utilizando spark.range().\n",
                                        "- Entender o início do fluxo de execução no PySpark."
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "5536055e",
                              "metadata": {},
                              "source": [
                                        "## Lista de Materiais\n",
                                        "\n",
                                        "- Computador com Anaconda (Jupyter Notebook) ou VS Code.\n",
                                        "- Ambiente com PySpark configurado.\n",
                                        "- Arquivo DADOSANO2C6B3S16A1_desenvolvendo_juntos.ipynb."
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "123c2544",
                              "metadata": {},
                              "source": [
                                        "### Etapa 1: Iniciando a SparkSession\n",
                                        "\n",
                                        "A SparkSession é o ponto de entrada do PySpark.\n",
                                        "Ela ativa o ambiente Spark para que possamos criar e manipular DataFrames."
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": 1,
                              "id": "35f25bbd",
                              "metadata": {},
                              "outputs": [
                                        {
                                                  "name": "stdout",
                                                  "output_type": "stream",
                                                  "text": [
                                                            "SparkSession iniciada.\n"
                                                  ]
                                        }
                              ],
                              "source": [
                                        "from pyspark.sql import SparkSession\n",
                                        "\n",
                                        "spark = (\n",
                                        "    SparkSession.builder\n",
                                        "    .appName(\"Aula1_DataFrame_range\")\n",
                                        "    .master(\"local[*]\")\n",
                                        "    .getOrCreate()\n",
                                        ")\n",
                                        "\n",
                                        "spark.sparkContext.setLogLevel(\"ERROR\")\n",
                                        "print(\"SparkSession iniciada.\")"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "f815545f",
                              "metadata": {},
                              "source": [
                                        "### O que fizemos aqui?\n",
                                        "\n",
                                        "- Criamos a SparkSession com um nome de aplicação.\n",
                                        "- Definimos o modo local (`local[*]`) para rodar no computador, usando os núcleos disponíveis.\n",
                                        "- A partir daqui, o ambiente está pronto para criar DataFrames."
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "9647dc62",
                              "metadata": {},
                              "source": [
                                        "### Etapa 2: Criando um DataFrame nativo do Spark\n",
                                        "\n",
                                        "Vamos utilizar `spark.range()` para criar um DataFrame diretamente no ambiente Spark.\n",
                                        "\n",
                                        "Esse método cria automaticamente uma coluna chamada **id**, contendo uma sequência numérica.\n",
                                        "\n",
                                        "Nesta etapa, o objetivo é compreender que o DataFrame já nasce dentro do Spark."
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": 2,
                              "id": "d9317ecf",
                              "metadata": {},
                              "outputs": [
                                        {
                                                  "name": "stdout",
                                                  "output_type": "stream",
                                                  "text": [
                                                            "DataFrame criado.\n",
                                                            "SparkSession iniciada com sucesso.\n"
                                                  ]
                                        }
                              ],
                              "source": [
                                        "df = spark.range(1, 11)  # cria id de 1 a 10\n",
                                        "print(\"DataFrame criado.\")\n",
                                        "print(\"SparkSession iniciada com sucesso.\")"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "065094b0",
                              "metadata": {},
                              "source": [
                                        "### O que aconteceu aqui?\n",
                                        "\n",
                                        "- Criamos um DataFrame com valores de 1 até 10.\n",
                                        "- O Spark organizou esses dados em formato tabular.\n",
                                        "- A coluna criada automaticamente se chama **id**.\n",
                                        "\n",
                                        "Perceba que já temos uma estrutura organizada, mesmo sem definir manualmente colunas."
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "5702be4f",
                              "metadata": {},
                              "source": [
                                        "### Etapa 3: Criando novas colunas no DataFrame\n",
                                        "\n",
                                        "Agora vamos expandir essa estrutura.\n",
                                        "\n",
                                        "Usaremos `withColumn()` para adicionar novas informações ao DataFrame:\n",
                                        "\n",
                                        "- Uma coluna de nome.\n",
                                        "- Uma coluna fixa de turma.\n",
                                        "- Uma coluna calculada de nota."
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": 3,
                              "id": "f304a062",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "from pyspark.sql import functions as F\n",
                                        "\n",
                                        "df = (\n",
                                        "    df\n",
                                        "    .withColumn(\"nome\", F.concat(F.lit(\"Aluno \"), F.col(\"id\")))\n",
                                        "    .withColumn(\"turma\", F.lit(\"C6\"))\n",
                                        "    .withColumn(\"nota\", (F.col(\"id\") * 0.8 + 2).cast(\"double\"))\n",
                                        ")"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "64cfb084",
                              "metadata": {},
                              "source": [
                                        "### O que fizemos nesta etapa?\n",
                                        "\n",
                                        "- `withColumn()` permite criar ou modificar colunas.\n",
                                        "- Criamos a coluna **nome** combinando texto com o valor do id.\n",
                                        "- Criamos a coluna **turma** com valor fixo.\n",
                                        "- Criamos a coluna **nota** com um cálculo matemático.\n",
                                        "\n",
                                        "Isso mostra que um DataFrame não é apenas uma lista de dados.\n",
                                        "Ele permite organizar e transformar informações de forma estruturada."
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "2f21523a",
                              "metadata": {},
                              "source": [
                                        "### Etapa 4: Executando uma ação\n",
                                        "\n",
                                        "Agora vamos visualizar o resultado.\n",
                                        "\n",
                                        "O método `show()` é uma ação.\n",
                                        "Ele faz o Spark executar o processamento e exibir os dados."
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": 4,
                              "id": "01bd1481",
                              "metadata": {},
                              "outputs": [
                                        {
                                                  "name": "stdout",
                                                  "output_type": "stream",
                                                  "text": [
                                                            "+---+--------+-----+------------------+\n",
                                                            "| id|    nome|turma|              nota|\n",
                                                            "+---+--------+-----+------------------+\n",
                                                            "|  1| Aluno 1|   C6|               2.8|\n",
                                                            "|  2| Aluno 2|   C6|               3.6|\n",
                                                            "|  3| Aluno 3|   C6|               4.4|\n",
                                                            "|  4| Aluno 4|   C6|               5.2|\n",
                                                            "|  5| Aluno 5|   C6|               6.0|\n",
                                                            "|  6| Aluno 6|   C6| 6.800000000000001|\n",
                                                            "|  7| Aluno 7|   C6|7.6000000000000005|\n",
                                                            "|  8| Aluno 8|   C6|               8.4|\n",
                                                            "|  9| Aluno 9|   C6|               9.2|\n",
                                                            "| 10|Aluno 10|   C6|              10.0|\n",
                                                            "+---+--------+-----+------------------+\n",
                                                            "\n"
                                                  ]
                                        }
                              ],
                              "source": [
                                        "df.show()"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "000b6318",
                              "metadata": {},
                              "source": [
                                        "### O que aconteceu ao executar show()?\n",
                                        "\n",
                                        "O Spark processou as transformações definidas anteriormente\n",
                                        "e exibiu o resultado na tela.\n",
                                        "\n",
                                        "Fluxo observado nesta aula:\n",
                                        "\n",
                                        "1. Criamos a SparkSession.\n",
                                        "2. Criamos um DataFrame.\n",
                                        "3. Adicionamos novas colunas.\n",
                                        "4. Executamos uma ação.\n",
                                        "\n",
                                        "Este é o início do fluxo de execução no PySpark.\n",
                                        "\n",
                                        "Na próxima aula, vamos aprofundar como inspecionar a estrutura do DataFrame."
                              ]
                    }
          ],
          "metadata": {
                    "kernelspec": {
                              "display_name": "base",
                              "language": "python",
                              "name": "python3"
                    },
                    "language_info": {
                              "codemirror_mode": {
                                        "name": "ipython",
                                        "version": 3
                              },
                              "file_extension": ".py",
                              "mimetype": "text/x-python",
                              "name": "python",
                              "nbconvert_exporter": "python",
                              "pygments_lexer": "ipython3",
                              "version": "3.12.4"
                    }
          },
          "nbformat": 4,
          "nbformat_minor": 5
}
